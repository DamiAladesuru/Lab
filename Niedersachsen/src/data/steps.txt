#######################################
1. load_maindata.py
#######################################
load main data
*get basic information like attributes, crs, head, etc.
preprocess data:
    create ne name for attributes with varying names across year e.g., year, kulturcode
    delete unneeded columns
    change year to datetime and int
    *see data info and head again
    load niederersachsen administrative boundary polygon to filter data in all years to just niedersachesen,
    thus deleting fields from other states which are in the data
add all years' data into one geodataframe
create additonal field-level data: area (m2- for shp metrics calculation, ha- visulaization), peri, shape index, fractal dimension
*see data info and head again. save data to shp so you can do visual inspeaction in another software like ArcGIS
run descriptives for the landscape over years. save (ldscp_desc)
load grid
reproject grid to 25382
left join grid to all_years data
identify and delete duplicates
save interim data as pickle (I saved as gld.pkl)

########################################
2. Move to desc_anal.py
########################################
load pkl file
create a dataframe (griddf) with unique grid cellcodes for all yeears
calculate grid statistics: counts, sums, mean, sd for area, perimeter, shape index and fractal dimension
populate griddf with these statistics
check for null values
do the descriptives of griddf and safe to csv (griddesc)
save griddf

########################################
    # Grid-Area weight for regional analysis = grid area/ sum of area of
    # fields in respective grid for each year
#all_years['area_wght'] = all_years.area / all_years.groupby('year')\
    # ['area_m2'].transform('sum')